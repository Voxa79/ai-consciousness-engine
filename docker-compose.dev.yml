services:
  api-gateway:
    build:
      context: ./api-gateway
      dockerfile: Dockerfile
      args:
        - CACHE_BUST=dev-2025-08-08-19h24
    image: api-gateway:dev
    environment:
      - PORT=3000
      - CONSCIOUSNESS_ENGINE_URL=http://mvp-server:4000
      - AGENT_ORCHESTRATOR_URL=http://mvp-server:4000
      - AI_GOVERNANCE_URL=http://mvp-server:4000
      - JWT_SECRET=dev-secret-key
    ports:
      - "3000:3000"
    depends_on:
      - mvp-server

  mvp-server:
    image: node:20-alpine
    working_dir: /app
    environment:
      - PORT=4000
      - NODE_ENV=development
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=tinyllama
      - VLM_MODEL=llava:7b
      - ASR_SERVICE_URL=http://asr:8000
      - TTS_SERVICE_URL=http://tts:5002
      - TTS_VOICE=fr-FR
    volumes:
      - ./.kiro/mvp-server:/app
    command: sh -c "npm install && npm run start"
    ports:
      - "4000:4000"
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_KEEP_ALIVE=5m
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama

  # Précharge des modèles adaptés au dev (léger à moyen)
  # - tinyllama: très rapide, idéal CI/smoke
  # - qwen2.5:3b-instruct: multilingue, raisonnable CPU
  # - llama3.2:3b-instruct: alternative 3B multilingue light
  # Note: Qwen3-235B-* est volontairement exclu (trop lourd pour cette stack)
  ollama-preload:
    image: curlimages/curl:8.6.0
    depends_on:
      - ollama
    entrypoint: ["/bin/sh","-lc"]
    command: >
      set -e; \
      echo "⏳ Attente du service Ollama..."; \
      for i in $(seq 1 60); do \
        if curl -sf http://ollama:11434/api/tags >/dev/null; then echo "✅ Ollama OK"; break; fi; \
        sleep 2; \
      done; \
      for MODEL in tinyllama qwen2.5:3b-instruct llama3.2:3b-instruct moondream2 llava:7b; do \
        echo "⬇️  Pull $MODEL"; \
        curl -s -X POST http://ollama:11434/api/pull -H 'Content-Type: application/json' -d "{\"name\":\"$MODEL\"}" || true; \
      done; \
      echo "✅ Préchargement terminé"; \
      tail -f /dev/null

  asr:
    image: node:18-alpine
    working_dir: /srv
    volumes:
      - ./.kiro/services/asr-mock:/srv
    command: sh -lc "(npm init -y >/dev/null 2>&1 || true) && npm install express --omit=dev --no-audit --no-fund && node server.js"
    environment:
      - PORT=8000
    ports:
      - "8000:8000"

  tts:
    image: node:18-alpine
    working_dir: /srv
    volumes:
      - ./.kiro/services/tts-mock:/srv
    command: sh -lc "(npm init -y >/dev/null 2>&1 || true) && npm install express --omit=dev --no-audit --no-fund && node server.js"
    environment:
      - PORT=5002
    ports:
      - "5002:5002"
  # Remplacez ces mocks par des images réelles (faster-whisper, Piper) en production.

  ultravox-adapter:
    image: node:20-alpine
    working_dir: /app
    environment:
      - PORT=8788
      - NODE_ENV=development
      - JWT_SECRET=dev-secret-key
      - ASR_SERVICE_URL=http://asr:8000
      - TTS_SERVICE_URL=http://tts:5002
      - GATEWAY_URL=http://api-gateway:3000
    volumes:
      - ./.kiro/services/ultravox-adapter:/app
    command: sh -lc "npm install && node server.js"
    ports:
      - "8788:8788"
    depends_on:
      - asr
      - tts
      - api-gateway

volumes:
  ollama_models:
